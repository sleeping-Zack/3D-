
**主题：Section 3.2 – Structured Latents Encoding and Decoding**

---

## 一、章节概述（Section Overview）

> *“With the structured latent representation, we develop an effective encoding scheme to encode 3D assets to it, and introduce different decoders for reconstruction across various 3D representations.”*

这一节介绍了论文核心框架中 **SLat (Structured Latents)** 的编码与解码机制：

* 如何从原始 3D 资产提取特征并编码为结构化潜伏向量 𝒛；
* 如何通过不同的解码器 𝓓 将 𝒛 还原为不同格式的 3D 资产（如 3D Gaussian、Radiance Field、Mesh 等）。

整体流程如下：

> **3D Asset → Visual Feature Aggregation → Sparse VAE Encoder (𝓔) → Structured Latents 𝒛 → Sparse VAE Decoder (𝓓) → 3D Outputs**

---

## 二、Visual Feature Aggregation（视觉特征聚合）

> *“We first convert each 3D asset 𝒪 into a voxelized feature 𝒇 = { (𝒇ᵢ, 𝒑ᵢ) }.i=1...L*

### ✦ 过程说明

1. **Voxelization 体素化**
   将三维物体 𝒪 离散化成由活跃体素 (𝒑ᵢ) 组成的 3D 网格；
2. **Multi-view Rendering 多视图渲染**
   在球面上随机采样多个视角，渲染出物体的多视图图像；
3. **Feature Extraction 特征提取**
   使用预训练的 **DINOv2 encoder [65]** 提取每张视图的图像特征；
4. **Projection & Aggregation 投影与聚合**
   将每个体素 𝒑ᵢ 投影到这些多视图特征图上，取对应位置特征的平均值（multiview average），得到局部视觉特征 𝒇ᵢ。

### ✦ 结果与意义

得到的体素化特征集合 𝒇 同时包含：

* 来自 active voxels 的**几何结构信息**；
* 来自 DINOv2 多视图特征的**外观与细节信息**。

> *“We set 𝒇 to match the resolution of the structured latents 𝒛 (i.e., 64³). Empirically, this is sufficient to reconstruct the original 3D asset at high fidelity.”*

这种特征融合方式在不增加计算成本的情况下，实现了高保真 3D 重建。

---

## 三、Sparse VAE for Structured Latents（稀疏 VAE 结构）

> *“Specifically, an encoder 𝓔 first encodes 𝒇 to structured latents 𝒛, followed by a decoder 𝓓 that converts 𝒛 into a 3D asset represented by certain 3D representation.”*

### ✦ 核心思路

* 采用基于 Transformer 的 **Sparse Variational Autoencoder (VAE)** 架构；
* 将体素化特征 𝒇 编码为结构化潜伏向量 𝒛 （即 SLat）；
* 解码器 𝓓 再将 𝒛 还原为目标 3D 表示（3DGS / RF / Mesh 等）。

### ✦ 训练目标

采用端到端（end-to-end）训练：

* **Reconstruction Loss 重建损失**：确保生成的 3D 资产与原始模型一致；
* **KL Penalty 正则项**：对潜伏变量 𝒛ᵢ 施加 KL 散度约束，使其服从正态分布。

---

## 四、Transformer Architecture 设计

> *“The encoder and decoder share the same transformer structure… To handle sparse voxels, we serialize input features from active voxels and add sinusoidal positional encodings…”*

### ✦ 编码器 / 解码器 共享结构

* 输入由所有 active voxels 的特征序列组成；
* 每个体素的空间位置通过 **sinusoidal positional encoding** 表示；
* 输入序列的长度 L 取决于活跃体素数量（稀疏，因此 L 可变）。

### ✦ 3D Shifted Window Attention

> *“… we incorporate shifted window attention in 3D space to enhance local information interaction, which also improves efficiency…”*

* 在 3D 空间引入 **Shifted Window Attention 机制**：

  * 聚焦局部邻域交互，提升几何细节捕捉能力；
  * 避免全局 self-attention 的高计算开销。
* 这种设计兼顾**效率（efficiency）**与**局部结构表达（local interaction）**。

---

## 五、整体流程总结

1. **输入**：3D 资产 → 多视图渲染 → DINOv2 特征提取 → 体素化特征 𝒇；
2. **编码**：Sparse VAE Encoder (𝓔) → 生成 Structured Latents 𝒛；
3. **解码**：Sparse VAE Decoder (𝓓) → 重建不同 3D 格式输出（3DGS / RF / Mesh）。

最终形成从 3D 资产到潜在空间再到可重建输出的统一流程。

